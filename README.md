# ğŸš€ ScrapeForce

**ScrapeForce** is a real-time web scraping and data pipeline project that automates the extraction of headlines from [Hacker News](https://news.ycombinator.com), transforms and stores the data in a PostgreSQL database, and visualizes it using Metabase. The pipeline is orchestrated using Apache Airflow and fully containerized using Docker Compose.

---

## âœ¨ Features

- ğŸ” Automated daily web scraping from Hacker News
- ğŸ§¼ Data cleaning and transformation
- ğŸ—ƒï¸ Storage in PostgreSQL database
- ğŸ“Š Interactive dashboards built with Metabase
- ğŸ“§ Email notifications on DAG success and failure
- ğŸ³ Fully containerized using Docker Compose
- ğŸ”„ Re-runnable and modular ETL design

---

## ğŸ§° Tech Stack

| Technology       | Purpose                          |
|------------------|----------------------------------|
| **Python**       | Scripting and ETL logic          |
| **Apache Airflow** | DAG scheduling and orchestration |
| **PostgreSQL**   | Relational data storage          |
| **Metabase**     | Data visualization dashboards    |
| **Docker Compose** | Container orchestration         |
| **BeautifulSoup**| HTML parsing for scraping        |
| **Pandas**       | Data transformation and analysis |

---

## ğŸ“ Project Structure
```
ScrapeForce/
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ scraping_dag.py # Airflow DAG for pipeline
â”‚ â””â”€â”€ scripts/
â”‚ â”œâ”€â”€ scrape.py # Scraping logic
â”‚ â”œâ”€â”€ transform.py # Data cleaning
â”‚ â””â”€â”€ load.py # Load to PostgreSQL
â”‚
â”œâ”€â”€ data/ # Optional: Output data (CSV)
â”‚
â”œâ”€â”€ Dockerfile.airflow # Custom Airflow image
â”œâ”€â”€ docker-compose.yaml # Container orchestration
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env # SMTP credentials for alerts
â””â”€â”€ README.md # Project documentation
```
---

## âš™ï¸ How to Get Started

### 1. ğŸ” Clone the Repository

```
git clone https://github.com/YOUR_USERNAME/ScrapeForce.git
cd ScrapeForce
```
### 2. ğŸ› ï¸ Configure Environment
```
Create a .env file in the root:
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password
SMTP_MAIL_FROM=your_email@gmail.com
```
### 3. ğŸ³ Build & Run the Containers
```
docker compose up --build
```
Once running:

Airflow UI: http://localhost:8080

Metabase UI: http://localhost:3000
### 4. ğŸ•¹ï¸ Trigger the DAG
```
Open Airflow UI

Enable and trigger the scrapeforce_web_scraping DAG

Scraped headlines will be saved to the PostgreSQL table scraped_data

```
--- 
### ğŸ“Š Sample Dashboard
Using Metabase:

Connect to the airflow database

Explore the scraped_data table

Create visualizations like:

Number of headlines scraped over time

Latest headlines

Trends per day

Save the visualizations to a dashboard

### ğŸŒ± Future Enhancements
âœ… Retry logic and error handling in scrapers

âœ… Multi-site scraping support

âœ… Support for parameterized scraping (topics, keywords)

âœ… Unit tests for ETL scripts using pytest

âœ… Deployment to AWS/GCP using ECS or Cloud Composer

âœ… Add Grafana/Prometheus for monitoring

### ğŸ‘¨â€ğŸ’» Author
Yash Pandey
ğŸ“§ pandeyash1706@gmail.com
